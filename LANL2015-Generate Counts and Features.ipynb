{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Base location of all data\n",
    "dataDirectory = 'D:\\\\LANL\\\\2015\\\\'\n",
    "\n",
    "# timestsamp to enable runtime measurement\n",
    "start = datetime.now()\n",
    "\n",
    "# Set the timeSlice to 10 minutes for feature generation\n",
    "timeSlice = 600 \n",
    "\n",
    "# Variables for calculating the number of seconds in an hour and day\n",
    "secHour = 3600\n",
    "secDay = 86400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DataFrame columns (as per descriptions in Cell 1)\n",
    "\n",
    "procCols = ['Time', 'User', 'Computer', 'ProcessName', 'Status']\n",
    "\n",
    "authCols = ['Time', 'User', 'DestUser', 'SourceComputer', 'DestComputer', 'AuthType', \\\n",
    "          'LogonType', 'AuthOrient', 'SuccessFail']\n",
    "\n",
    "redCols = ['Time', 'User', 'SourceComputer', 'DestComputer']\n",
    "\n",
    "# Open raw files as pandas DataFrames (opening proc and auth data in chunks due to their size)\n",
    "\n",
    "chunkSize = 1e6\n",
    "\n",
    "procDataChunks = pd.read_csv(dataDirectory + 'Raw\\\\proc.txt.gz', compression='gzip', \\\n",
    "                           chunksize=chunkSize, names=procCols)\n",
    "\n",
    "authDataChunks = pd.read_csv(dataDirectory + 'Raw\\\\auth.txt.gz', compression='gzip', \\\n",
    "                           chunksize=chunkSize, names=authCols)\n",
    "\n",
    "redDf = pd.read_csv(dataDirectory + 'Raw\\\\redteam.txt.gz', names=redCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cycle through proccess events and generate counts and feature vectors for analysis and modelling\n",
    "\n",
    "lastHourDf = pd.DataFrame(columns = procCols)\n",
    "dayFeatureDf = pd.DataFrame()\n",
    "currentDay=1\n",
    "\n",
    "for chunk in procDataChunks:\n",
    "    \n",
    "    # Add new Day, Hour and Timeslice columns\n",
    "    chunk['Day'] = chunk['Time'].apply(lambda x : int((x/secDay)+1))\n",
    "    chunk['Hour'] = chunk['Time'].apply(lambda x : int((x/secHour)%24))\n",
    "    chunk['Slice'] = chunk['Time'].apply(lambda x : int(x/timeSlice))\n",
    "    chunk = chunk.astype({'Day': 'int', 'Hour': 'int', 'Slice': 'int'})\n",
    "    \n",
    "    print('Processing Process Events Day: ' + str(int(chunk['Day'].max())) + ', \\\n",
    "    Hour: ' + str(int(chunk['Hour'].min())))\n",
    "    \n",
    "    # Get incomplete final day/hour from last chunk and append all complete hours in this chunk \n",
    "    lastHour=chunk[chunk['Day'] == chunk.Day.max()].Hour.max()\n",
    "    procChunk = lastHourDf.append(chunk[chunk['Hour'] != lastHour]) \n",
    "        \n",
    "    # Generate features for User activity by time slice (computers used and processes launched)\n",
    "    featureVector = procChunk.groupby(['Day', 'Hour', 'Slice', 'User']).agg({ \\\n",
    "                                       'ProcessName': [lambda x: set(x), 'count'], \\\n",
    "                                       'Computer': [lambda x: set(x), 'count']})\n",
    "    \n",
    "    featureVector = featureVector.reset_index()\n",
    "    featureVector.columns = ['Day', 'Hour', 'Slice', 'User', 'Processes', 'ProcCnt',\\\n",
    "                             'Computers', 'CompCnt']\n",
    "   \n",
    "    featureVector = featureVector.astype({'Day': 'int', 'Hour': 'int', 'Slice': 'int', 'ProcCnt': 'int', \\\n",
    "                             'CompCnt': 'int'})\n",
    "    featureVector['UniqueProcCnt'] = featureVector['Processes'].apply(lambda x: len(x))\n",
    "    featureVector['UniqueCompCnt'] = featureVector['Computers'].apply(lambda x: len(x))\n",
    "    \n",
    "    dayFeatureDf = dayFeatureDf.append(featureVector[featureVector['Day']==currentDay])\n",
    "    if featureVector.Day.max() > currentDay:\n",
    "        meanValues = dayFeatureDf.groupby(['User']).agg({'UniqueProcCnt': [np.nanmean], \\\n",
    "                                                            'UniqueCompCnt': [np.nanmean], \\\n",
    "                                                            'ProcCnt': [np.nanmean], \\\n",
    "                                                            'CompCnt': [np.nanmean]})\n",
    "        meanValues = meanValues.reset_index()\n",
    "        meanValues.columns = ['User', 'UniqueProcCntMean', 'UniqueCompCntMean', 'ProcCntMean', 'CompCntMean']\n",
    "        meanValues = meanValues.astype({'UniqueProcCntMean': 'float', 'UniqueCompCntMean': 'float', \\\n",
    "                                        'ProcCntMean': 'float', 'CompCntMean': 'float'})\n",
    "        \n",
    "        dayFeatureDf = dayFeatureDf.merge(meanValues, how='outer', left_on=['User'], right_on=['User'])\n",
    "        \n",
    "        dayFeatureDf['ProcCntNorm'] = dayFeatureDf['ProcCnt'] / dayFeatureDf['ProcCntMean']\n",
    "        dayFeatureDf['CompCntNorm'] = dayFeatureDf['CompCnt'] / dayFeatureDf['CompCntMean']\n",
    "        dayFeatureDf['UniqueProcCntNorm'] = dayFeatureDf['UniqueProcCnt'] / dayFeatureDf['UniqueProcCntMean']\n",
    "        dayFeatureDf['UniqueCompCntNorm'] = dayFeatureDf['UniqueCompCnt'] / dayFeatureDf['UniqueCompCntMean']\n",
    "        \n",
    "        dayFeatureDf.to_pickle(dataDirectory + 'Analysis\\\\Day_' + str(currentDay).zfill(2) + '_procFeat.pkl', \\\n",
    "                               compression='gzip')\n",
    "        currentDay+=1\n",
    "        dayFeatureDf = featureVector[featureVector['Day']==currentDay]\n",
    "    \n",
    "    # Get last incomplete hour of this chunk to append to the next chunk\n",
    "    lastHourDf = chunk[chunk['Hour'] == lastHour]\n",
    "    \n",
    "    \n",
    "print('Process Logs Complete...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cycle through authentication events and generate counts and feature vector data for analysis and modelling\n",
    "\n",
    "lastHourDf = pd.DataFrame(columns = authCols)\n",
    "dayFeatureDf = pd.DataFrame()\n",
    "currentDay=1\n",
    "\n",
    "for chunk in authDataChunks:\n",
    "    \n",
    "    # Add new Day, Hour and Timeslice columns and limit AuthType field length\n",
    "    chunk['Day'] = chunk['Time'].apply(lambda x : int((x/secDay)+1))\n",
    "    chunk['Hour'] = chunk['Time'].apply(lambda x : int((x/secHour)%24))\n",
    "    chunk['Slice'] = chunk['Time'].apply(lambda x : int(x/timeSlice))\n",
    "    chunk = chunk.astype({'Day': 'int', 'Hour': 'int', 'Slice': 'int'})\n",
    "    chunk['AuthType'] = chunk['AuthType'].apply(lambda x: x[:24] if len(x) > 24 else x)\n",
    "    \n",
    "    print('Processing Authentication Events Day: ' + str(int(chunk['Day'].max())) + ', \\\n",
    "    Hour: ' + str(int(chunk['Hour'].min())))\n",
    "    \n",
    "    # Get incomplete final day/hour from last chunk and append all complete hours in this chunk \n",
    "    lastHour=chunk[chunk['Day'] == chunk.Day.max()].Hour.max()\n",
    "    authChunk = lastHourDf.append(chunk[chunk['Hour'] != lastHour])  \n",
    "    \n",
    "    # Generate features for User activity by time slice\n",
    "    featureVector = authChunk.groupby(['Day', 'Hour', 'Slice', 'User']).agg({ \\\n",
    "                                        'DestUser': [lambda x: set(x),'count'], \\\n",
    "                                        'DestComputer': [lambda x: set(x), 'count'], \\\n",
    "                                        'SourceComputer': [lambda x: set(x), 'count'],\n",
    "                                        'AuthType': [lambda x: set(x)], \\\n",
    "                                        'LogonType': [lambda x: set(x)], \\\n",
    "                                        'AuthOrient': [lambda x: set(x)], \\\n",
    "                                        'SuccessFail': [ \\\n",
    "                                        lambda x: x[x.values=='Success'].count(), \\\n",
    "                                        lambda x: x[x.values=='Fail'].count()]})\n",
    "    \n",
    "    featureVector = featureVector.reset_index()\n",
    "    featureVector.columns = ['Day', 'Hour', 'Slice', 'User', 'DestUsers', 'DestUserCnt', \\\n",
    "                   'DestComputers', 'DestCompCnt', 'SourceComputers', 'SourceCompCnt', \\\n",
    "                   'AuthTypes', 'LogonTypes', 'AuthOrients', 'SuccessfulLogonCnt', 'FailedLogonCnt'] \n",
    "    \n",
    "    featureVector = featureVector.astype({'Day': 'int', 'Hour': 'int', 'Slice': 'int', 'DestUserCnt': 'int', \\\n",
    "                             'DestCompCnt': 'int', 'SourceCompCnt': 'int', 'SuccessfulLogonCnt': 'int', \\\n",
    "                                'FailedLogonCnt': 'int'})\n",
    "    \n",
    "    featureVector['UniqueDestUserCnt'] = featureVector['DestUsers'].apply(lambda x: len(x))\n",
    "    featureVector['UniqueSourceCompCnt'] = featureVector['SourceComputers'].apply(lambda x: len(x))\n",
    "    featureVector['UniqueDestCompCnt'] = featureVector['DestComputers'].apply(lambda x: len(x))\n",
    "    featureVector['AuthKerberos'] = featureVector['AuthTypes'].apply(lambda x: 'Kerberos' in str(x))\n",
    "    featureVector['AuthNTLM'] = featureVector['AuthTypes'].apply(lambda x: 'NTLM' in str(x))\n",
    "    featureVector['AuthMicrosoft'] = featureVector['AuthTypes'].apply(lambda x: 'MICROSOFT' in str(x))\n",
    "    featureVector['AuthNegotiate'] = featureVector['AuthTypes'].apply(lambda x: 'Negotiate' in str(x))\n",
    "    featureVector['AuthUnknown'] = featureVector['AuthTypes'].apply(lambda x: '?' in str(x))\n",
    "    featureVector['LogonNet'] = featureVector['LogonTypes'].apply(lambda x: 'Network' in str(x))\n",
    "    featureVector['LogonService'] = featureVector['LogonTypes'].apply(lambda x: 'Service' in str(x))\n",
    "    featureVector['LogonClearText'] = featureVector['LogonTypes'].apply(lambda x: 'Cleartext' in str(x))\n",
    "    featureVector['LogonBatch'] = featureVector['LogonTypes'].apply(lambda x: 'Batch' in str(x))\n",
    "    featureVector['LogonInteractive'] = featureVector['LogonTypes'].apply(lambda x: 'Interactive' in str(x))\n",
    "    featureVector['LogonUnlock'] = featureVector['LogonTypes'].apply(lambda x: 'Unlock' in str(x))\n",
    "    featureVector['LogonCreds'] = featureVector['LogonTypes'].apply(lambda x: 'Credentials' in str(x))\n",
    "    featureVector['LogonUnknown'] = featureVector['LogonTypes'].apply(lambda x: '?' in str(x))\n",
    "    featureVector['OrientLogon'] = featureVector['AuthOrients'].apply(lambda x: 'LogOn' in str(x))\n",
    "    featureVector['OrientLogoff'] = featureVector['AuthOrients'].apply(lambda x: 'LogOff' in str(x))\n",
    "    featureVector['OrientTGS'] = featureVector['AuthOrients'].apply(lambda x: 'TGS' in str(x))\n",
    "    featureVector['OrientTGT'] = featureVector['AuthOrients'].apply(lambda x: 'TGT' in str(x))\n",
    "    featureVector['OrientAuthMap'] = featureVector['AuthOrients'].apply(lambda x: 'AuthMap' in str(x))\n",
    "    featureVector['Machine'] = featureVector['User'].str.contains('^[^U]')\n",
    "     \n",
    "    dayFeatureDf = dayFeatureDf.append(featureVector[featureVector['Day']==currentDay])\n",
    "    if featureVector.Day.max() > currentDay:\n",
    "        meanValues = dayFeatureDf.groupby(['User']).agg({'UniqueDestUserCnt': [np.nanmean], \\\n",
    "                                                            'UniqueSourceCompCnt': [np.nanmean], \\\n",
    "                                                            'UniqueDestCompCnt': [np.nanmean]})\n",
    "        meanValues = meanValues.reset_index()\n",
    "        meanValues.columns = ['User', 'UniqueDestUserMean', 'UniqueSourceCompMean', 'UniqueDestCompMean']\n",
    "        meanValues = meanValues.astype({'UniqueDestUserMean': 'float', 'UniqueSourceCompMean': 'float', \\\n",
    "                                        'UniqueDestCompMean': 'float'})\n",
    "        \n",
    "        dayFeatureDf = dayFeatureDf.merge(meanValues, how='outer', left_on=['User'], right_on=['User'])\n",
    "        \n",
    "        dayFeatureDf['UniqueDestUserNorm'] = dayFeatureDf['UniqueDestUserCnt']/ \\\n",
    "        dayFeatureDf['UniqueDestUserMean']\n",
    "        dayFeatureDf['UniqueSourceCompNorm'] = dayFeatureDf['UniqueSourceCompCnt'] / \\\n",
    "        dayFeatureDf['UniqueSourceCompMean']\n",
    "        dayFeatureDf['UniqueDestCompNorm'] = dayFeatureDf['UniqueDestCompCnt'] / \\\n",
    "        dayFeatureDf['UniqueDestCompMean']\n",
    "        \n",
    "        \n",
    "        dayFeatureDf.to_pickle(dataDirectory + 'Analysis\\\\Day_' + str(currentDay).zfill(2) + '_authFeat.pkl', \\\n",
    "                              compression='gzip')\n",
    "        currentDay+=1\n",
    "        dayFeatureDf = featureVector[featureVector['Day']==currentDay]\n",
    "        \n",
    "    # Get last incomplete hour of this chunk to append to the next chunk\n",
    "    lastHourDf = chunk[chunk['Hour'] == lastHour]\n",
    "    \n",
    "print('Authentication Logs Complete...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the process entity counts\n",
    "\n",
    "timeOffset = datetime(2015,1,1,0,0).timestamp()\n",
    "lastHourDf = pd.DataFrame(columns = ['Time', 'Human', 'User', 'Computer', 'ProcessName', 'Status'])\n",
    "currentDay=1\n",
    "\n",
    "countDf = pd.DataFrame(columns = ['Time', 'Humans', 'Users', 'Computers', 'Processes'])\n",
    "countDf.to_csv(dataDirectory + 'Analysis\\\\procEntityCounts.csv', index=False, header=True)\n",
    "\n",
    "for chunk in procDataChunks:\n",
    "    \n",
    "    # Add new Day, Hour and Timeslice columns\n",
    "    chunk['Day'] = chunk['Time'].apply(lambda x : int((x/secDay)+1))\n",
    "    chunk['Hour'] = chunk['Time'].apply(lambda x : int((x/secHour)%24))\n",
    "    chunk['Slice'] = chunk['Time'].apply(lambda x : int(x/timeSlice))\n",
    "    chunk = chunk.astype({'Day': 'int', 'Hour': 'int', 'Slice': 'int'})\n",
    "    chunk['Human'] = chunk['User'].apply(returnUser)\n",
    "    \n",
    "    print('Processing Process Events Day: ' + str(int(chunk['Day'].max())) + ', \\\n",
    "    Hour: ' + str(int(chunk['Hour'].min())))\n",
    "    \n",
    "    # Get incomplete final day/hour from last chunk and append all complete hours in this chunk \n",
    "    lastHour=chunk[chunk['Day'] == chunk.Day.max()].Hour.max()\n",
    "    procChunk = lastHourDf.append(chunk[chunk['Hour'] != lastHour]) \n",
    "    \n",
    "    # Generate unique User, Computer and Process counts for each hour\n",
    "    countDf = procChunk.groupby(['Day', 'Hour']).agg({\n",
    "        'Time': [lambda x: str(pd.Timestamp(datetime.utcfromtimestamp(timeOffset+x.min()-(x.min()%secHour))))], \\\n",
    "        'Human': ['nunique'], 'User': ['nunique'], 'Computer': ['nunique'], 'ProcessName': ['nunique']})\n",
    "    \n",
    "    countDf.to_csv(dataDirectory + 'Analysis\\\\procEntityCounts.csv', mode='a', index=False, header=False)\n",
    "    \n",
    "    lastHourDf = chunk[chunk['Hour'] == lastHour]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the authorisation entity counts\n",
    "\n",
    "timeOffset = datetime(2015,1,1,0,0).timestamp()\n",
    "lastHourDf = pd.DataFrame(columns = ['Time', 'Human', 'User', 'DestUser', 'SourceComputer', 'DestComputer', 'AuthType', \\\n",
    "          'LogonType', 'AuthOrient', 'SuccessFail'])\n",
    "currentDay=1\n",
    "\n",
    "countDf = pd.DataFrame(columns = ['Time', 'Humans', 'Users', 'Dest Users', 'Source Computers', 'Dest Computers'])\n",
    "countDf.to_csv(dataDirectory + 'Analysis\\\\authEntityCounts.csv', index=False, header=True)\n",
    "\n",
    "\n",
    "for chunk in authDataChunks:\n",
    "    \n",
    "    # Add new Day, Hour and Timeslice columns and limit AuthType field length\n",
    "    chunk['Day'] = chunk['Time'].apply(lambda x : int((x/secDay)+1))\n",
    "    chunk['Hour'] = chunk['Time'].apply(lambda x : int((x/secHour)%24))\n",
    "    chunk['Slice'] = chunk['Time'].apply(lambda x : int(x/timeSlice))\n",
    "    chunk = chunk.astype({'Day': 'int', 'Hour': 'int', 'Slice': 'int'})\n",
    "    chunk['AuthType'] = chunk['AuthType'].apply(lambda x: x[:24] if len(x) > 24 else x)\n",
    "    chunk['Human'] = chunk['User'].apply(returnUser)\n",
    "    \n",
    "    print('Processing Authentication Events Day: ' + str(int(chunk['Day'].max())) + ', \\\n",
    "    Hour: ' + str(int(chunk['Hour'].min())))\n",
    "    \n",
    "    # Get incomplete final day/hour from last chunk and append all complete hours in this chunk \n",
    "    lastHour=chunk[chunk['Day'] == chunk.Day.max()].Hour.max()\n",
    "    authChunk = lastHourDf.append(chunk[chunk['Hour'] != lastHour])  \n",
    "    \n",
    "     # Generate unique User, Computer and Process counts for each hour\n",
    "    countDf = authChunk.groupby(['Day', 'Hour']).agg({ \\\n",
    "            'Time': [lambda x: str(pd.Timestamp(datetime.utcfromtimestamp(timeOffset+x.min()-(x.min()%secHour))))], \\\n",
    "            'Human': ['nunique'], 'User': ['nunique'], 'DestUser': ['nunique'], 'SourceComputer': ['nunique'], \\\n",
    "            'DestComputer': ['nunique']})\n",
    "    \n",
    "    countDf.to_csv(dataDirectory + 'Analysis\\\\authEntityCounts.csv', mode='a', index=False, header=False)\n",
    "    \n",
    "    lastHourDf = chunk[chunk['Hour'] == lastHour]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
